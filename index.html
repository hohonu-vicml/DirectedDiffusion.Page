<!DOCTYPE html>
<html>



  <head>

 <style>
hr {
        border: 0;
    height: 1px;
<!-- background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0)); -->
}
.bottom-three {
     margin-bottom: 1cm;
  }
</style>

  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Directed Diffusion: Direct Control of Object Placement through Attention Guidance
    </title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL">
    </script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
    </script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
</script>
<script type="text/javascript"
  src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js">
    </script>
    <script defer src="./static/js/fontawesome.all.min.js">
    </script>
    <script src="./static/js/bulma-carousel.min.js">
    </script>
    <script src="./static/js/bulma-slider.min.js">
    </script>
    <script src="./static/js/index.js">
    </script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true">
      </span>
      <span aria-hidden="true">
      </span>
      <span aria-hidden="true">
      </span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://github.com/hohonu-vicml/">
      <span class="icon">
        <i class="fas fa-home">
        </i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hohonu-vicml.github.io/Trailblazer.Page">
            TrailBlazer
          </a>
          <!-- <a class="navbar-item" href="https://nerfies.github.io"> -->
          <!--   Nerfies -->
          <!-- </a> -->
          <!-- <a class="navbar-item" href="https://latentfusion.github.io"> -->
          <!--   LatentFusion -->
          <!-- </a> -->
          <!-- <a class="navbar-item" href="https://photoshape.github.io"> -->
          <!--   PhotoShape -->
          <!-- </a> -->
        </div>
      </div>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Directed Diffusion: Direct Control of Object Placement through Attention Guidance
          </h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://www.linkedin.com/in/kurt-ma/">Wan-Duo Kurt Ma
              </a>
              <sup>1
              </sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en">Avisek Lahiri
              </a>
              <sup>2
              </sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="http://www.scribblethink.org/">J. P. Lewis
              </a>
              <sup>3*
              </sup>,&nbsp&nbsp
            </span>
            <span class="author-block">
              <a href="https://research.google/people/ThomasLeung/">Thomas Leung
              </a>
              <sup>2
              </sup>,
            </span>
            <span class="author-block">
              <a href="https://people.wgtn.ac.nz/bastiaan.kleijn">W. Bastiaan Kleijn
              </a>
              <sup>1,2
              </sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <sup>1
              </sup>Victoria University of Wellington,&nbsp&nbsp
            </span>
            <span class="author-block">
              <sup>2
              </sup>Google Research,&nbsp&nbsp
            </span>
            <span class="author-block">
              <sup>3
              </sup>NVIDIA Research
            </span>
            <br>
            <span class="author-block">
              <sup>*
              </sup>Work done at Google Research
            </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2302.13153.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf">
                    </i>
                  </span>
                  <span>Paper
                  </span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2302.13153"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv">
                    </i>
                  </span>
                  <span>arXiv
                  </span>
                </a>
              </span>
              <!-- Video Link. -->
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=L4ijMG1mXGI"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-youtube">
                    </i>
                  </span>
                  <span>Project
                  </span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/hohonu-vicml/DirectedDiffusion.Page/blob/master/assets/AAAI_poster.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                   <i class="far fa-images"></i>
                    </i>
                  </span>
                  <span>Poster
                  </span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block"> -->
              <!--   <a href="https://www.youtube.com/watch?v=P-PSkS7sNco" -->
              <!--      class="external-link button is-normal is-rounded is-dark"> -->
              <!--     <span class="icon"> -->
              <!--         <i class="fab fa-youtube"></i> -->
              <!--     </span> -->
              <!--     <span>Result</span> -->
              <!--   </a> -->
              <!-- </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/hohonu-vicml/DirectedDiffusion"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github">
                    </i>
                  </span>
                  <span>Code
                  </span>
                  </a>
              </span>
              <!-- demo Link. -->
              <!-- <span class="link-block"> -->
              <!--   <a href="https://huggingface.co/spaces/hohonu-vicml/Trailblazer" -->
              <!--      class="external-link button is-normal is-rounded is-dark"> -->
              <!--     <span class="icon"> -->
              <!--       <i class="far fa-images"></i> -->
              <!--     </span> -->
              <!--     <span>Demo(NEW!)</span> -->
              <!--     </a> -->
              <!-- </span> -->
             <!-- <\!-- dataset Link. -\-> -->
            <!--   <span class="link-block"> -->
            <!--     <a href="https://github.com/google/nerfies/releases/tag/0.1" -->
            <!--        class="external-link button is-normal is-rounded is-dark"> -->
            <!--       <span class="icon"> -->
            <!--           <i class="far fa-images"></i> -->
            <!--       </span> -->
            <!--       <span>Data</span> -->
            <!--       </a> -->
            <!-- </div> -->

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img class="center pixelated" src="assets/images/teaser-cropped-4.png"
          style="display:inline-block;width:200%;position:relative"
          onmouseover="this.src='assets/images/teaser-cropped-4.png';"
          onmouseout="this.src='assets/images/teaser-cropped-4.png';">
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Directed Diffusion (DD)
        </span>  augments denoising diffusion text-to-image
       generation by allowing the position of specified objects to be controlled
       with user-specified bounding boxes highlighted in red.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract
        </h2>
        <div class="content has-text-justified">
          <p>  Text-guided diffusion models such as DALLE-2, Imagen, eDiff-I, and Stable
   Diffusion are able to generate an effectively endless variety of images given
   only a short text prompt describing the desired image content. In many cases
   the images are of very high quality. However, these models often struggle to
   compose scenes containing several key objects such as characters in specified
   positional relationships. The missing capability to ``direct'' the placement
   of characters and objects both within and across images is crucial in
   storytelling, as recognized in the literature on film and animation theory.
   In this work, we take a particularly straightforward approach to providing
   the needed direction. Drawing on the observation that the cross-attention
   maps for prompt words reflect the spatial layout of objects denoted by those
   words, we introduce an optimization objective that produces ``activation'' at
   desired positions in these cross-attention maps. The resulting approach is a
   step toward generalizing the applicability of text-guided diffusion models
   beyond single images to collections of related images, as in storybooks.
   Directed Diffusion provides easy high-level positional control over multiple
   objects, while making use of an existing pre-trained model and maintaining a
   coherent blend between the positioned objects and the background. Moreover,
            it requires only a few lines to implement.
          </p>
        </div>
      </div>
    </div>
</section>



<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- <\!-- Visual Effects. -\-> -->
      <!-- <div class="column"> -->
      <!--   <div class="content"> -->
      <!--     <h2 class="title is-3">Visual Effects</h2> -->
      <!--     <p> -->
      <!--       Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect -->
      <!--       would be impossible without nerfies since it would require going through a wall. -->
      <!--     </p> -->
      <!--     <video id="dollyzoom" autoplay controls muted loop playsinline height="100%"> -->
      <!--       <source src="./static/videos/dollyzoom-stacked.mp4" -->
      <!--               type="video/mp4"> -->
      <!--     </video> -->
      <!--   </div> -->
      <!-- </div> -->
      <!-- <\!--/ Visual Effects. -\-> -->

      <!-- <\!-- Matting. -\-> -->
      <!-- <div class="column"> -->
      <!--   <h2 class="title is-3">Matting</h2> -->
      <!--   <div class="columns is-centered"> -->
      <!--     <div class="column content"> -->
      <!--       <p> -->
      <!--         As a byproduct of our method, we can also solve the matting problem by ignoring -->
      <!--         samples that fall outside of a bounding box during rendering. -->
      <!--       </p> -->
      <!--       <video id="matting-video" controls playsinline height="100%"> -->
      <!--         <source src="./static/videos/matting.mp4" -->
      <!--                 type="video/mp4"> -->
      <!--       </video> -->
      <!--     </div> -->
      <!--   </div> -->
      <!-- </div> -->


    </div>
    <!--/ Matting. -->

    <!-- Animation. -->
    <div class="columns is-centered">
      <div class="column is-full-width">

        <!-- Core method -->
        <h2 class="title is-3">Update
        </h2>
        <div class="content has-text-justified">
         Our Directed Diffusion has been accepted and published at the <b>AAAI 2024</b> conference. We are actively working on enhancing our codebase, which is currently at the first ArXiv version. The updates are scheduled for release in <b>March 2024</b>, along with an online HuggingFace demo.
        </div>
        <hr class="bottom-three">

        <!-- Core method -->
        <h2 class="title is-3">Background
        </h2>
        <div class="content has-text-justified">

        <div class="content has-text-justified">
<p style="text-align:justify">
DD builds on the following observation:
The overall position and shape of a synthesized objects appears near
the beginning of the diffusion denoising process, while the final steps do not change this
but keep refining the details.
</p>
</div>

<div class="content has-text-justified">
<figure>
  <img class="center" src="assets/images/recons.gif" style="display:inline-block;width:30%;">
  <img class="center" src="assets/images/SD-inter-render.png" style="display:inline-block;width:48%;">
  <img class="center pixelated" src="assets/images/ca.gif" style="display:inline-block;width:100%;position:relative;right:0%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    Intermediate reconstructions and visualizations of prompt token cross-attention
    maps. Prompt:
    <i>A cat sitting on a car.
    </i> <strong>(Top-left)</strong> VAE reconstruction from the predicted noise at each step;
    <strong>(Bottom)</strong> The cross attention associated with the six words in the prompt;
    <strong>(Top-right)</strong> The denoising progress at steps 50,40,25,0. The cat's position
    and shape is established early in the denoising, as seen in the red box at
    step 40.
  </figcaption>
</figure>
</div>







        <hr class="bottom-three">
        <!-- Compositing -->
        <h2 class="title is-3">Approach
        </h2>
        <div class="content has-text-justified">
          <p>
            DD can be divided into two stages:
            <i>Attention Editing
            </i> and
            <i>Convention SD Denoising
            </i>.
            Our
            <b>Attention Editing
            </b>
            <i>directly
            </i> strengthens and weakens the
activations in a specified region of selected cross-attention maps through optimization.
            <i>Trailing attention maps
            </i> are those that do not correspond to words in the
prompt. These play a central role in establishing consistency between the
subjects and the surrounding environment shown in the image. The optimal number
of trailing attention maps are found by minimizing our objective that
contributes the best directed object in the prompt attention maps.
</p>
        </div>
        <div class="content has-text-centered">
          <figure>
  <img class="center pixelated" src="assets/images/diagram-pipeline-v3.png" style="display:inline-block;width:100%;position:relative;right:0%">
  <!-- <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--      Directed Diffusion (DD) pipeline overview: DD divides the denoising process into initial steps where Attention -->
<!-- Editing is performed, followed by refinement steps using Conventional SD Denoising. The goal of the Attention Editing -->
<!-- stage is to optimize the cross attention map for a directed word (red, outlined in green) to approximately match a target -->
<!-- D in which neural “activation” has been injected with a Gaussian fall-off inside a bounding box specified by the user. -->
<!-- The optimization is performed by adjusting a vector (a, green) that re-weights the trailing attention maps. Note that the -->
<!-- optimization objective involves successive two denosing time steps. Please see the text for details. -->
<!--   </figcaption> -->
</figure>
        </div>



        <hr class="bottom-three">
        <!-- Pipelin -->
        <h2 class="title is-3">Key Result - One object: Four quadrants
        </h2>
        <div class="content has-text-justified">
          <p>
            The following clip shows the directed object allocating in each of
  four image quadrants as well as at the image center, with the directed
  object <b>a large cabin</b>, <b>an insect robot</b>, and <b>the sun</b>,
  compared with conventional SD result shown at beginning.
          </p>
        </div>

        <div class="content has-text-centered">
          <figure>
<video class="center"
       style="display:inline-block;width:80%;position:relative;right:0%;top:-5%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-4q.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<!-- <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--     DD results directed by the green bounding box compared with a SD result for -->
<!--     the same prompt. Prompt: (Left) -->
<!--     <i>“A large cabin on top of a sunny mountain -->
<!--       in the style of Dreamworks artstation. -->
<!--     </i> (Middle) -->
<!--     <i>An insect robot -->
<!--       preparing a delicious meal and food in the kitchen. -->
<!--     </i> (Right) -->
<!--     <i>The sun -->
<!--       shines on a house. -->
<!--     </i> -->
<!-- </figcaption> -->
</figure>
</div>


        <h2 class="title is-3">Key Results - One object: Sliding window
        </h2>
        <div class="content has-text-justified">
          <p>
            The following clip shows the moving object from left to right. All
the results are generated with a bounding box with the full image height and 40%
of image width, with the directed object <b>A stone castle</b>, <b>A white cat</b>,
<b>A white dog</b>, compared with conventional SD result shown at beginning.
          </p>
        </div>

        <div class="content has-text-centered">
<figure>
<video class="center"
       style="display:inline-block;width:80%;position:relative;right:0%;top:-5%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-window.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<!-- <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--     DD results directed by the green bounding box compared with a SD result for the same prompt. -->
<!--     Prompt: (Left) -->
<!--     <i>A stone castle surrounded by lakes and trees -->
<!--     </i> (Middle) -->
<!--     <i>A white cat sitting on a car. -->
<!--     </i> -->
<!--     (Right) -->
<!--     <i>A white dog waking on the street in the city. -->
<!--     </i> -->
<!-- </figcaption> -->
</figure>
</div>



        <h2 class="title is-3">Key Results - Placement Finetuning
        </h2>
        <div class="content has-text-justified">
          <p>
            In some cases the artist may wish to experiment with different object positions
after obtaining a desirable image. However, when the object’s bounding box is
moved DD may generate a somewhat different object. Our <b>Placement Finetuning (PF)</b>
method addresses this problem, allowing the artist to immediately experiment
with different positions for an object while keeping its identity, and without
requiring any model fine-tuning or other optimization.
          </p>
        </div>

        <div class="content has-text-centered">
<figure>
<video class="center" width="1200"
       style="display:inline-block;width:90%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-pf.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<!-- <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!-- The following experiment compares the sliding window result shown above -->
<!-- (Middle), and the placement finetuning (Right) which takes the original DD -->
<!-- result as the image source (Left). -->
<!-- </figcaption> -->
</figure>
</div>



        <h2 class="title is-3">Key Results - Two bounding boxes
        </h2>
        <div class="content has-text-justified">

          <p style="text-align:justify">
 <b>Compositionality</b> is a well known failure case for SD, e.g. the
  <a style="text-decoration:none;"
     href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">Huggingface
  </a>
  page specifically mentions the example of “A red cube on top of a blue sphere”.
  Here we use DD to successfully produce this example,
  by using two bounding boxes for the sphere and cube respectively.
</p>

        </div>

        <div class="content has-text-centered">

<video class="center" width="1200"
       style="display:inline-block;width:60%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-sphere-cube.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>

<video class="center" width="1200"
       style="display:inline-block;width:60%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-bear.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
</div>




<!--         <h3>Comparison - Methods validation -->
<!--         </h3> -->

<!-- <p style="text-align:justify"> -->
<!-- we showcase our results alongside the results obtained from the recommended -->
<!-- configurations from the BLD Avrahami et al. [2022a], and the public Composable -->
<!-- Diffusion (CD) Nan et al. [2022], implementations of BoxDiff Xie et al. [2023] and GLIGEN Li et -->
<!-- al. [2023]. -->

<!-- The results are picked from a list of experiments from different random seeds -->
<!-- with the best-of-k image rule subjectively. Please refer to our paper and the -->
<!-- appendix for more detail and the comprehensive comparisons. -->
<!-- </p> -->

<!-- <b>Scene compositionality -->
<!-- </b> -->

<!-- <p style="text-align:justify"> -->
<!-- Our results demonstrate better fidelity to the prompt (castle with fog, pond is -->
<!-- dark) and less information blending between objects (e.g., church and mountain -->
<!-- are not pink). -->
<!-- </p> -->

<!-- <figure> -->
<!--   <img class="center pixelated" src="assets/images/exp-app-comp-v3.png" style="display:inline-block;width:100%;position:relative;right:0%"> -->
<!--   <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--     <b>Composition comparison -->
<!--     </b>: Results from CD, GLIGEN, BOXDIFF, and our DD. The -->
<!-- directed object is highlighted in bold in the prompt and its bounding box is -->
<!-- shown in green (except for CD, which does not support position guidance). Please -->
<!-- enlarge to see details including the bounding box. -->
<!--   </figcaption> -->
<!-- </figure> -->


<!-- <b>One-Masking interaction -->
<!-- </b> -->

<!-- <p style="text-align:justify"> -->
<!-- In our results it is evident that DD generates strong and realistic interactions -->
<!-- between the directed object and the background, including the man touching the -->
<!-- gravestone, the shadows cast on the grass, the occlusion of the volcano by the -->
<!-- horse. Oh no, the man’s hand that catches on fire :) -->
<!-- </p> -->

<!-- <figure> -->
<!--   <img class="center pixelated" src="assets/images/exp-app-mask-v3.png" style="display:inline-block;width:100%;position:relative;right:0%"> -->
<!--   <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--     <b>Masking comparison -->
<!--     </b>:  Results from CD, GLIGEN, BOXDIFF, and our DD. The -->
<!-- directed object is highlighted in bold in the prompt and its bounding box is -->
<!-- shown in green (except for CD, which does not support position guidance). Please -->
<!-- enlarge to see details including the bounding box. BLD images are reproduced -->
<!-- from Avrahami et al. [2022a]. -->
<!--   </figcaption> -->
<!-- </figure> -->


<!-- <h3>Comparison: Object interaction from prompt verbs -->
<!-- </h3> -->

<!-- The following figure showcases the ability of DD to (sometimes) convey -->
<!-- interaction between objects. The figure compares two prompts: "A white dog and a -->
<!-- red ball" and "A white dog chasing a red ball." While both results show the -->
<!-- white dog and red ball consistently, it is evident that the “chasing” prompt -->
<!-- better conveys the action of this verb, e.g. the dog is running rather than -->
<!-- sitting. -->

<!-- <figure> -->
<!--   <img class="center pixelated" src="assets/images/exp-chasing.jpg" style="display:inline-block;width:100%;position:relative;right:0%"> -->
<!--   <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;"> -->
<!--    Results of the prompt “a white dog S* a red ball”, where the token S* is -->
<!-- replaced by “and” or “chasing” in the first row and second row, respectively. In -->
<!-- the second row the dog more frequently appears to be running rather than -->
<!-- sitting, consistent with the verb “chasing”. Note the results of each column -->
<!-- share the same random seed. -->
<!--   </figcaption> -->
<!-- </figure> -->

<hr class="bottom-three">
  <h2 class="title is-3">Limitations
  </h2>
        <div class="content has-text-justified">
          <p style="text-align:justify">
            While Directed Diffusion can be used to address Stable Diffusion's problem with composing multiple objects, it inherits Stable Diffusion's other limitations -- in particular the need for experimentation with prompts and hyperparameters.
            <a href="https://www.forbes.com/sites/tjmccue/2023/02/23/mind-bending-midjourney-ai-art-and-images-from-simple-prompts/amp">
              <i>"This is the thing with AI tools right now - it takes a ton of time and a willingness to keep experimenting."
              </i>
            </a>
            In practice our method would be used in conjunction with other algorithms such as Dreambooth. The examples in the paper do not make use of these other algorithms in order to provide a stand-alone evaluation of our algorithm.
          </p>
        </div>



        <h2 class="title is-3">Societal Impact
        </h2>
        <div class="content has-text-justified">
          <p style="text-align:justify">
            The aim of this project is to extend the capability of text-to-image
models to visual storytelling. In common with most other technologies, there is
potential for misuse. In particular, generative models reflect the biases of
their training data, and there is a risk that malicious parties can use
text-to-image methods to generate misleading images. The authors believe that it
is important that these risks be addressed. This might be done by penalizing
malicious behavior though the introduction of appropriate laws, or by limiting
the capabilities of generative models to serve this behavior.
          </p>
        </div>


        <h2 class="title is-3">Contribution
        </h2>
<div class="content has-text-justified">
We summarize the contributions of Directed Diffusion:
<ul>
  <li>
    <b>Storytelling
    </b>: Our method is a first step towards storytelling by providing consistent control over the positioning of multiple objects.
  </li>
  <li>
    <b>Compositionality
    </b>: It provides an alternate and direct approach to ''compositionality'' by providing explicit positional control.
  </li>
  <li>
    <b>Consistency
    </b>: The positioned objects seamlessly and consistently
    fit in the environment, rather than appearing as a splice from another image
with inconsistent interaction (shadows, lighting, etc.) This consistency is due
to two factors.
  </li>
  <li>
    <b>Simplicity
    </b>: From a computational point of view, our method requires no training or
fine tuning, and can be added to an existing text-driven diffusion model with
cross-attention guidance with only a few lines of code. It requires a simple
    optimization of a small weight vector $\mathbf{a} ∈ \mathbb{R}^d$, $d
    < 77$,
which does not significantly increase the overall synthesis time. </li>
</ul>
</p>
</div>

<h2 class="title is-3">Acknowledgements
</h2>
<div class="content has-text-justified">
<p>
We thank Jason Baldridge, and the anonymous reviewers for helpful feedback. We
also thank Huggingface public project Diffusers providing efficient and reliable
experiments.
</p>
</div>

<h2 class="title is-3">Citation
</h2>
<div class="content has-text-justified">
 <p>
            If you find our <span class="dnerf">DirectedDiffusion</span> interesting,
            please consider citing our article.
          </p>
</div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{ma2023directed,
          title={Directed Diffusion: Direct Control of Object Placement through Attention Guidance},
          author={Wan-Duo Kurt Ma and J. P. Lewis and Avisek Lahiri and Thomas Leung and W. Bastiaan Kleijn},
          year={2023},
          eprint={2302.13153},
          archivePrefix={arXiv},
          primaryClass={cs.CV}
      }
    </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/hohonu-vicml" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website utilizes the page template developed by
            the <a href="https://nerfies.github.io">Nerfies</a> team. The
            authors extend their appreciation for their diligent efforts in
            creating this highly flexible and high-quality work.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
