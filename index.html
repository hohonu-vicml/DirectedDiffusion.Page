<!--
  Copyright 2018 The Distill Template Authors

  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

       http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<!doctype html>

<style>
.base-grid,
pa.n-header,
.n-byline,
.n-title,
.n-article,
.n-footer {
    display: grid;
    justify-items: stretch;
    grid-template-columns: [screen-start] 8px [page-start kicker-start text-start gutter-start middle-start] 1fr 1fr 1fr 1fr 1fr 1fr 1fr 1fr [text-end page-end gutter-end kicker-end middle-end] 8px [screen-end];
    grid-column-gap: 8px;
    border: 0;
}

.grid {
  display: grid;
  grid-column-gap: 8px;
}

@media(min-width: 768px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start middle-start text-start] 45px 45px 45px 45px 45px 45px 45px 45px [ kicker-end text-end gutter-start] 45px [middle-end] 45px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media(min-width: 1000px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 50px [middle-start] 50px [text-start kicker-end] 50px 50px 50px 50px 50px 50px 50px 50px [text-end gutter-start] 50px [middle-end] 50px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 16px;
    }

    .grid {
        grid-column-gap: 16px;
    }
}

@media (min-width: 180px) {
    .base-grid,
    .n-header,
    .n-byline,
    .n-title,
    .n-article,
    .n-footer {
        display: grid;
        justify-items: stretch;
        grid-template-columns: [screen-start] 1fr [page-start kicker-start] 60px [middle-start] 60px [text-start kicker-end] 60px 60px 60px 60px 60px 60px 60px 60px [text-end gutter-start] 60px [middle-end] 60px [page-end gutter-end] 1fr [screen-end];
        grid-column-gap: 56px;
    }
    .grid {
        grid-column-gap: 56px;
    }

}

/* unvisited link */
a:link {
  color: blue;
}

/* visited link */
a:visited {
  color: blue;
}

/* mouse over link */
a:hover {
  color: hotpink;
}

/* selected link */
a:active {
  color: blue;
}

.base-grid {
  grid-column: screen;
}

/* default grid column assignments */
.n-title > *  {
    grid-column: text;
}

.n-article > *  {
  grid-column: text;
}

.n-title {
    padding: 2.5rem 0 0;
}

.l-page {
    grid-column: page;
}

.l-article {
    grid-column: text;
}

p {
  margin-top: 0;
  margin-bottom: 1em;
}

.pixelated {
    image-rendering: pixelated;
}

strong {
    font-weight: 600;
    }


/*------------------------------------------------------------------*/
/* title */
.n-title h1 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 40px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0;
    text-align: center;
}

.n-title h2 {
    font-family: "Barlow",system-ui,Arial,sans-serif;
    color:#082333;
    grid-column: text;
    font-size: 30px;
    font-weight: 700;
    line-height: 1.1em;
    margin: 0 0 0;
    text-align: center;
}

@media (min-width: 768px) {
    .n-title h1 {
        font-size: 50px;
    }
}


.n-byline {
  contain: style;
  overflow: hidden;
  /* border-top: 1px solid rgba(0, 0, 0, 0.1); */
  font-size: 0.8rem;
  line-height: 1.8em;
  /* padding: 1.5rem 0; */
  min-height: 1.8em;
}

.n-byline .byline {
  grid-column: text;
}

.byline {
    grid-template-columns: 1fr 1fr 1fr 1fr;
}

.grid {
    display: grid;
    grid-column-gap: 8px;
}

.hover_img {

position: absolute;
top: 0;
left: 0;
display: none;

}

@media (min-width: 768px) {
.grid {
    grid-column-gap: 16px;
}
}

.n-byline p {
  margin: 0;
}

.n-byline h3 {
    font-size: 0.6rem;
    font-weight: 400;
    color: rgba(0, 0, 0, 0.5);
    margin: 0;
    text-transform: uppercase;
}
.n-byline .authors-affiliations {
  grid-column-end: span 2;
  grid-template-columns: 1fr 1fr;
}

ul.authors {
  list-style-type: none;
  padding: 0;
  margin: 0;
  text-align: center;
  contain: style;
  overflow: hidden;
  /* border-top: 1px solid rgba(0, 0, 0, 0.1); */
  font-size: 1.2rem;
  line-height: 1.8em;
  padding: 1.5rem 0;
  min-height: 1.8em;
}
ul.authors li {
    padding: 0 0.5rem;
    display: inline-block;
}

ul.authors sup {
    color: rgb(126,126,126);
}

ul.authors.affiliations  {
    margin-top: -2.5rem;
}

ul.authors.affiliations li {
    color: rgb(126,126,126);
}

.preload { visibility: hidden; }

* {box-sizing:border-box}

/* Slideshow container */
.panorama-slideshow {
  position: relative;
}

/* Hide the images by default */
.panorama-slide {
  display: none;
}

/* Hide the images by default */
div[class^='image-gallery-slide-'] {
  display: none;
}

/* Next & previous buttons */
.prev, .next {
  cursor: pointer;
  position: absolute;
  top: 50%;
  width: auto;
  margin-top: -22px;
  padding: 16px;
  color: white;
  font-weight: bold;
  font-size: 25px;
  transition: 0.1s ease;
  border-radius: 0 2px 2px 0;
  user-select: none;
}

/* Position the "next button" to the right */
.next {
  right: 0;
  border-radius: 3px 0 0 3px;
}

/* On hover, add a black background color with a little bit see-through */
.prev:hover, .next:hover {
  background-color: rgba(0,0,0,0.8);
}


/* Next & previous buttons */
.prev-image, .next-image {
  cursor: pointer;
  position: absolute;
  top: 50%;
  width: auto;
  margin-top: -22px;
  margin-left: -50px;
  margin-right: -30px;
  padding: 16px;
  color: black;
  font-weight: bold;
  font-size: 40px;
  transition: 0.6s ease;
  border-radius: 0 3px 3px 0;
  user-select: none;
}

/* Position the "next button" to the right */
.next-image {
  right: 0;
  border-radius: 3px 0 0 3px;
}


.prev-image:hover, .next-image:hover {
  background-color: rgba(0,0,0,0.8);
  color: white;
}

/* Caption text */
.text {
  color: #f2f2f2;
  font-size: 15px;
  padding: 8px 12px;
  position: absolute;
  bottom: 8px;
  width: 100%;
  text-align: center;
}



/* Fading animation */
.fade {
  -webkit-animation-name: fade;
  -webkit-animation-duration: 1.5s;
  animation-name: fade;
  animation-duration: 1.5s;
}

@-webkit-keyframes fade {
  from {opacity: .4}
  to {opacity: 1}
}

@keyframes fade {
  from {opacity: .4}
  to {opacity: 1}
}

/* Style tab links */
.tablink {
  background-color: #fff;
  color: black;
  float: left;
  outline: none;
  cursor: pointer;
  padding: 8px 5px;
  font-size: 17px;
  font-weight: bold;
  border: none;

}

.tablink:hover {
  background-color: #36373A;
  color: white;
}

/* Style the tab content (and add height:100% for full page content) */
.tabcontent {
  color: white;
  display: none;
  padding: 100px 20px;
  height: 100%;
}

@media screen and (min-width: 601px) {
  .tablink {
    font-size: 17px;
  }
}

@media screen and (max-width: 600px) {
  .tablink {
    font-size: 12px;
  }
}
</style>


<head>
    <title>Directed Diffusion</title>
    <script src="assets/page/template.v2.js"></script>
    <meta property="og:title" content="Directed Diffusion">
    <meta property="og:type" content="website">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <meta charset="utf8">
    <script>
        $(window).on( "load", function(){
        $('.preload').attr('src', function(i,a){
        $(this).attr('src','')
            .removeClass('preload')
            .attr('src', a);
        });
      });
    </script>
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script>
    <script type="text/javascript"
            src="http://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
</head>

<body>



<!--  ____             _             _              -->
<!-- | __ )  ___  __ _(_)_ __  _ __ (_)_ __   __ _  -->
<!-- |  _ \ / _ \/ _` | | '_ \| '_ \| | '_ \ / _` | -->
<!-- | |_) |  __/ (_| | | | | | | | | | | | | (_| | -->
<!-- |____/ \___|\__, |_|_| |_|_| |_|_|_| |_|\__, | -->
<!--             |___/                       |___/  -->
<d-article>
<div style="center">
   <div class="byline">
   <h1 style="font-size:350%"><center>Directed Diffusion:</center></h1>
   <h2 style="font-size:180%"><center>Direct Control of Object Placement through Attention Guidance</center></h2>
    <ul class="authors">
      <li><a href="https://www.linkedin.com/in/kurt-ma">Wan-Duo Kurt Ma<sup>1</sup></a></li>
      <li><a href="https://research.google/people/JPLewis/">J. P. Lewis<sup>2*</sup></a></li>
      <li><a href="https://scholar.google.co.in/citations?user=4zgNd2UAAAAJ&hl=en">Avisek Lahiri<sup>2</sup></a></li>
      <li><a href="https://research.google/people/ThomasLeung/">Thomas Leung<sup>2</sup></a></li>
      <li><a href="https://people.wgtn.ac.nz/bastiaan.kleijn">W. Bastiaan Kleijn<sup>1,2</sup></a></li>
    </ul>
    <ul class="authors affiliations">
      <li><sup>1</sup> Victoria University of Wellington, Wellington, New Zealand</li>
      <li><sup>2</sup> Google Research, Mountain View, USA</li>
      <li><sup>*</sup> Current affiliation: NVIDIA Research</li>
    </ul>
   </div>


</div>





  <figure>
     <img class="center pixelated" src="assets/images/teaser-cropped-4.png"
          style="display:inline-block;width:150%;position:relative;right:25%"
          onmouseover="this.src='assets/images/teaser-cropped-4.png';"
          onmouseout="this.src='assets/images/teaser-cropped-4.png';">
     <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
       <b>Directed Diffusion (DD)</b> augments denoising diffusion text-to-image
       generation by allowing the position of specified objects to be controlled
       with user-specified bounding boxes (highlighted in red). (Left) DD
       generates specified objects (insect robot, cat) placed according to the
       given bounding boxes. We can move a synthesized object (middle), and
       place multiple objects in desired locations (right). All the directed
       objects show the appropriate ``contextual'' interaction (e.g. shadows)
       with the background.
     </figcaption>
   </figure>


   <h3><center>Abstract</center></h3>

  <p style="text-align:justify">
   Text-guided diffusion models such as DALLE-2, Imagen, eDiff-I, and Stable
   Diffusion are able to generate an effectively endless variety of images given
   only a short text prompt describing the desired image content. In many cases
   the images are of very high quality. However, these models often struggle to
   compose scenes containing several key objects such as characters in specified
   positional relationships. The missing capability to ``direct'' the placement
   of characters and objects both within and across images is crucial in
   storytelling, as recognized in the literature on film and animation theory.
   In this work, we take a particularly straightforward approach to providing
   the needed direction. Drawing on the observation that the cross-attention
   maps for prompt words reflect the spatial layout of objects denoted by those
   words, we introduce an optimization objective that produces ``activation'' at
   desired positions in these cross-attention maps. The resulting approach is a
   step toward generalizing the applicability of text-guided diffusion models
   beyond single images to collections of related images, as in storybooks.
   Directed Diffusion provides easy high-level positional control over multiple
   objects, while making use of an existing pre-trained model and maintaining a
   coherent blend between the positioned objects and the background. Moreover,
   it requires only a few lines to implement.
   </p>
<!-- R25LR0 -->

<!--  ____             _                                   _  -->
<!-- | __ )  __ _  ___| | ____ _ _ __ ___  _   _ _ __   __| | -->
<!-- |  _ \ / _` |/ __| |/ / _` | '__/ _ \| | | | '_ \ / _` | -->
<!-- | |_) | (_| | (__|   < (_| | | | (_) | |_| | | | | (_| | -->
<!-- |____/ \__,_|\___|_|\_\__, |_|  \___/ \__,_|_| |_|\__,_| -->
<!--                       |___/                              -->
<h3>Background</h3>

<p style="text-align:justify">
There are notable issues regarding scene composition in Stable Diffusion (SD). First
of all, it is difficult to individually control different objects' appearance using the prompt.
Second, it is not possible to control an object's location using the prompt.
</p>

Our work provides a method to direct object location in images generated with Stable Diffusion.
This is achieved without any training or fine-tuning.
<!-- (and potentially other text-to-image methods that use text guidance through cross attention).-->
<p>

<figure>
  <img class="center pixelated" src="assets/images/SD-problem_v3.png" style="display:inline-block;width:100%;">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    <b>SD results</b>. Prompts from left: <i>A bird landing on top of a house</i> (Note:
    no house); <i>A bird flying over a house coming from the left</i> (Note: difficult
    to judge the position); <i>A yellow Aston Martin DB5 on a bridge</i> (Note: no
    bridge); <i>A yellow Aston Martin DB5 on a bridge under flocks of birds</i> (Note:
    no bridge or birds)
  </figcaption>
</figure>

<p style="text-align:justify">
DD builds on the following observation:
The overall position and shape of a synthesized objects appears near
the beginning of the diffusion denoising process, while the final steps do not change this
but keep refining the details.
</p>

<figure>
  <img class="center pixelated" src="assets/images/recons.gif" style="display:inline-block;width:39%;">
  <img class="center pixelated" src="assets/images/SD-inter-render.png" style="display:inline-block;width:59%;">
  <img class="center pixelated" src="assets/images/ca.gif" style="display:inline-block;width:130%;position:relative;right:15%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    Intermediate reconstructions and visualizations of prompt token cross-attention
    maps. Prompt: <i>A cat sitting on a car.</i> (Top-left) VAE reconstruction from the
    predicted noise at each step; (Bottom) The cross attention associated with the
    six words in the prompt; (Top-right) The denoising progress at steps 50,40,25,0.
    The cat's position and shape is established early in the denoising, as seen
    in the red box at step 40;
    (Middle-right) The initial (step 50) and final (step 0) cross-attention maps for each
    word in the prompt.
  </figcaption>
</figure>

<!--     _                                     _      -->
<!--    / \   _ __  _ __  _ __ ___   __ _  ___| |__   -->
<!--   / _ \ | '_ \| '_ \| '__/ _ \ / _` |/ __| '_ \  -->
<!--  / ___ \| |_) | |_) | | | (_) | (_| | (__| | | | -->
<!-- /_/   \_\ .__/| .__/|_|  \___/ \__,_|\___|_| |_| -->
<!--         |_|   |_|                                -->
<h3>Approach</h3>

<p style="text-align:justify">
DD can be divided into two stages:
<i>Attention Editing</i> and <i>Convention SD Denoising</i>.
In the Attention Editing stage, DD directs object position by manipulating activation in
the text guidance cross-attention layers.
</p>


<p style="text-align:justify">
Our <b>Attention Editing</b> <i>directly</i> strengthens and weakens the
activations in a specified region of selected cross-attention maps through optimization.
<i>Trailing attention maps</i> are those that do not correspond to words in the
prompt. These play a central role in establishing consistency between the
subjects and the surrounding environment shown in the image. The optimal number
of trailing attention maps are found by minimizing our objective that
contributes the best directed object in the prompt attention maps.
</p>

<figure>
  <img class="center pixelated" src="assets/images/diagram-pipeline-v3.png" style="display:inline-block;width:150%;position:relative;right:25%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
     Directed Diffusion (DD) pipeline overview: DD divides the denoising process into initial steps where Attention
Editing is performed, followed by refinement steps using Conventional SD Denoising. The goal of the Attention Editing
stage is to optimize the cross attention map for a directed word (red, outlined in green) to approximately match a target
D in which neural “activation” has been injected with a Gaussian fall-off inside a bounding box specified by the user.
The optimization is performed by adjusting a vector (a, green) that re-weights the trailing attention maps. Note that the
optimization objective involves successive two denosing time steps. Please see the text for details.
  </figcaption>
</figure>

</p>

<!--  ____                 _ _        -->
<!-- |  _ \ ___  ___ _   _| | |_ ___  -->
<!-- | |_) / _ \/ __| | | | | __/ __| -->
<!-- |  _ <  __/\__ \ |_| | | |_\__ \ -->
<!-- |_| \_\___||___/\__,_|_|\__|___/ -->
<h3 style="position:relative;right:25%">Key Results - One object: Four quadrants</h3>
<p style="text-align:justify">
  The following clip shows the directed object allocating in each of four image
  quadrants as well as at the image center, with the directed object ``a large
  cabin'', ``an insect robot'', and ``the sun''.

</p>
<figure>
<video class="center"
       style="display:inline-block;width:100%;position:relative;right:0%;top:-5%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-4q.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    DD results directed by the green bounding box compared with a SD result for
    the same prompt. Prompt: (Left) <i>“A large cabin on top of a sunny mountain
    in the style of Dreamworks artstation.</i> (Middle) <i>An insect robot
    preparing a delicious meal and food in the kitchen.</i> (Right) <i>The sun
    shines on a house.</i>
</figcaption>
</figure>


<!--  ___ _ _    _ _         __      ___         _ -->
<!-- / __| (_)__| (_)_ _  __ \ \    / (_)_ _  __| |_____ __ __ -->
<!-- \__ \ | / _` | | ' \/ _` \ \/\/ /| | ' \/ _` / _ \ V  V / -->
<!-- |___/_|_\__,_|_|_||_\__, |\_/\_/ |_|_||_\__,_\___/\_/\_/ -->
<!--                     |___/ -->

<h3 style="position:relative;right:25%">Key Results - One object: Sliding window</h3>
<p style="text-align:justify">
The following clip shows the moving object from left to right. All the results
are generated with a bounding box with the full image height and 40% of image
width, with the directed object ``A stone castle'', ``A white cat'', ``A white dog''.
</p>

<figure>
<video class="center"
       style="display:inline-block;width:100%;position:relative;right:0%;top:-5%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-window.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    DD results directed by the green bounding box compared with a SD result for the same prompt.
    Prompt: (Left) <i>A stone castle surrounded by lakes and trees</i> (Middle) <i>A white cat sitting on a car.</i>
    (Right) <i>A white dog waking on the street in the city.</i>
</figcaption>
</figure>



<!--  ___ _                           _   ___ _          _ -->
<!-- | _ \ |__ _ __ ___ _ __  ___ _ _| |_| __(_)_ _  ___| |_ _  _ _ _  ___ -->
<!-- |  _/ / _` / _/ -_) '  \/ -_) ' \  _| _|| | ' \/ -_)  _| || | ' \/ -_) -->
<!-- |_| |_\__,_\__\___|_|_|_\___|_||_\__|_| |_|_||_\___|\__|\_,_|_||_\___| -->

<h3 style="position:relative;right:25%">Key Results - Placement Finetuning</h3>

<p style="text-align:justify">
In some cases the artist may wish to experiment with different object positions
after obtaining a desirable image. However, when the object’s bounding box is
moved DD may generate a somewhat different object. Our placement finetuning (PF)
method addresses this problem, allowing the artist to immediately experiment
with different positions for an object while keeping its identity, and without
requiring any model fine-tuning or other optimization.

</p>
<figure>
<video class="center" width="1200"
       style="display:inline-block;width:100%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-pf.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>
<figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
The following experiment compares the sliding window result shown above
(Middle), and the placement finetuning (Right) which takes the original DD
result as the image source (Left).
</figcaption>
</figure>




<!--  _____            ___ -->
<!-- |_   _|_ __ _____| _ ) _____ _____ ___ -->
<!--   | | \ V  V / _ \ _ \/ _ \ \ / -_|_-< -->
<!--   |_|  \_/\_/\___/___/\___/_\_\___/__/ -->

<h3 style="position:relative;right:25%">Key Results - Two bounding boxes</h3>



<p style="text-align:justify">
   It is often necessary to directly control the position
and interaction of two objects, as in the case where a character is interacting with another character or object in some
specified environment, however control over exact placement of background environment objects is rarely necessary or
   desirable.
</p>
<p style="text-align:justify">
  "Compositionality" is a well known failure case for SD, e.g. the
  <a style="text-decoration:none;"
  href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">Huggingface</a>
  page specifically mentions the example of “A red cube on top of a blue sphere”.
  Here we use DD to successfully produce this example,
  by using two bounding boxes for the sphere and cube respectively.
</p>



<video class="center" width="1200"
       style="display:inline-block;width:100%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-sphere-cube.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>

<video class="center" width="1200"
       style="display:inline-block;width:100%;position:relative;right:0%"
       playsinline autoplay loop muted>
  <source src="assets/images/web-bear.mov" type="video/mp4" />
  <source src="movie.ogg" type="video/ogg" />
  Your browser does not support the video tag.
</video>



<h3 style="position:relative;right:25%">Comparison - Methods validation</h3>

<p style="text-align:justify">
we showcase our results alongside the results obtained from the recommended
configurations from the BLD Avrahami et al. [2022a], and the public Composable
Diffusion (CD) Nan et al. [2022], implementations of BoxDiff Xie et al. [2023] and GLIGEN Li et
al. [2023].

The results are picked from a list of experiments from different random seeds
with the best-of-k image rule subjectively. Please refer to our paper and the
appendix for more detail and the comprehensive comparisons.
</p>

<b>Scene compositionality</b>

<p style="text-align:justify">
Our results demonstrate better fidelity to the prompt (castle with fog, pond is
dark) and less information blending between objects (e.g., church and mountain
are not pink).
</p>

<figure>
  <img class="center pixelated" src="assets/images/exp-app-comp-v3.png" style="display:inline-block;width:100%;position:relative;right:0%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    <b>Composition comparison</b>: Results from CD, GLIGEN, BOXDIFF, and our DD. The
directed object is highlighted in bold in the prompt and its bounding box is
shown in green (except for CD, which does not support position guidance). Please
enlarge to see details including the bounding box.
  </figcaption>
</figure>


<b>One-Masking interaction</b>

<p style="text-align:justify">
In our results it is evident that DD generates strong and realistic interactions
between the directed object and the background, including the man touching the
gravestone, the shadows cast on the grass, the occlusion of the volcano by the
horse. Oh no, the man’s hand that catches on fire :)
</p>

<figure>
  <img class="center pixelated" src="assets/images/exp-app-mask-v3.png" style="display:inline-block;width:100%;position:relative;right:0%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
    <b>Masking comparison</b>:  Results from CD, GLIGEN, BOXDIFF, and our DD. The
directed object is highlighted in bold in the prompt and its bounding box is
shown in green (except for CD, which does not support position guidance). Please
enlarge to see details including the bounding box. BLD images are reproduced
from Avrahami et al. [2022a].
  </figcaption>
</figure>


<h3 style="position:relative;right:25%">Comparison: Object interaction from prompt verbs</h3>

The following figure showcases the ability of DD to (sometimes) convey
interaction between objects. The figure compares two prompts: "A white dog and a
red ball" and "A white dog chasing a red ball." While both results show the
white dog and red ball consistently, it is evident that the “chasing” prompt
better conveys the action of this verb, e.g. the dog is running rather than
sitting.

<figure>
  <img class="center pixelated" src="assets/images/exp-chasing.jpg" style="display:inline-block;width:100%;position:relative;right:0%">
  <figcaption style="margin-top: 0; margin-bottom: 0; text-align: left;">
   Results of the prompt “a white dog S* a red ball”, where the token S* is
replaced by “and” or “chasing” in the first row and second row, respectively. In
the second row the dog more frequently appears to be running rather than
sitting, consistent with the verb “chasing”. Note the results of each column
share the same random seed.
  </figcaption>
</figure>






<!--   ___         _       _ _         _   _ -->
<!--  / __|___ _ _| |_ _ _(_) |__ _  _| |_(_)___ _ _ -->
<!-- | (__/ _ \ ' \  _| '_| | '_ \ || |  _| / _ \ ' \ -->
<!--  \___\___/_||_\__|_| |_|_.__/\_,_|\__|_\___/_||_| -->
<h3>Contribution</h3>
<p style="margin-bottom: 4%; margin-top: -2%">
We summarize the contributions of Directed Diffusion:
<ul>
  <li> <b>Storytelling</b>: Our method is a first step towards storytelling by providing consistent control over the positioning of multiple objects. </li>
  <li> <b>Compositionality</b>: It provides an alternate and direct approach to ''compositionality'' by providing explicit positional control. </li>
  <li> <b>Consistency</b>: The positioned objects seamlessly and consistently
    fit in the environment, rather than appearing as a splice from another image
with inconsistent interaction (shadows, lighting, etc.) This consistency is due
to two factors. First, we use a simple bounding box to position and allow the
denoising diffusion process to fill in the details, whereas specifying position
with a pixel-space mask runs the risk that it may be inconsistent with the shape
and position implicit in the early denoising results. Second, subsequent
diffusion steps operate on the entire image and seamlessly produce consistent
lighting, etc.</li>
  <li> <b>Simplicity</b>: Image editing methods for text-to-image models often
require detailed masks, depth maps, or other precise guidance information. This
information is not available when synthesizing images ab initio, and it would be
    laborious to create. Our method allows the user to control the desired locations
of objects simply by specifying approximate bounding boxes. From a computational
point of view, our method requires no training or fine tuning, and can be added
to an existing text-driven diffusion model with cross-attention guidance with
only a few lines of code. It requires a simple optimization of a small weight
vector $\mathbf{a} ∈ \mathbb{R}^d$, $d < 77$, which does not significantly
increase the overall synthesis time. </li>
</ul>
</p>



<!--  _    _       _ _        _   _ -->
<!-- | |  (_)_ __ (_) |_ __ _| |_(_)___ _ _ -->
<!-- | |__| | '  \| |  _/ _` |  _| / _ \ ' \ -->
<!-- |____|_|_|_|_|_|\__\__,_|\__|_\___/_||_| -->
<h3>Limitations</h3>
<p style="text-align:justify">
While Directed Diffusion can be used to address Stable Diffusion's problem with composing multiple objects, it inherits Stable Diffusion's other limitations -- in particular the need for experimentation with prompts and hyperparameters.
<a href="https://www.forbes.com/sites/tjmccue/2023/02/23/mind-bending-midjourney-ai-art-and-images-from-simple-prompts/amp">
<i>"This is the thing with AI tools right now - it takes a ton of time and a willingness to keep experimenting."</i></a>
In practice our method would be used in conjunction with other algorithms such as Dreambooth. The examples in the paper do not make use of these other algorithms in order to provide a stand-alone evaluation of our algorithm.
</p>


<h3>Societal Impact</h3>
<p style="text-align:justify">
The aim of this project is to extend the capability of text-to-image models to
visual storytelling. In common with most other technologies, there is potential
for misuse. In particular, generative models reflect the biases of their
training data, and there is a risk that malicious parties can use text-to-image
methods to generate misleading images. The authors believe that it is important
that these risks be addressed. This might be done by penalizing malicious
behavior though the introduction of appropriate laws, or by limiting the
capabilities of generative models to serve this behavior.
</p>



<!--   ___         _ -->
<!--  / __|___  __| |___ -->
<!-- | (__/ _ \/ _` / -_) -->
<!--  \___\___/\__,_\___| -->

<h3>Code</h3>
<p style="text-align:justify">
Our public implementation
(Github: <a href="https://github.com/hohonu-vicml/DirectedDiffusion">link</a>) is
still associated with our first ArXiv version (DD ArXiv v1:
<a href="https://arxiv.org/abs/2302.13153v1">link</a>). This version has the
core method injecting the Gaussian weight map into the cross-attention map as
one of our main motivation in the paper. We will soon provide the online demo
through Huggingface Space and sync with the latest paper version in the near future. Feel free to contact us if you
have comments and suggestions!
<!-- Additionally, we also released an online demo app on Huggingface Space running on A10 GPU (<a href="https://huggingface.co/spaces/hohonu-vicml/DirectedDiffusion">link</a>). -->
</p>


<h3>Acknowledgements</h3>
<p style="text-align:justify">
We thank Jason Baldridge, and the anonymous reviewers for helpful feedback. We
also thank Huggingface public project Diffusers providing efficient and reliable
experiments.
</p>

<!--   ___ _ _        _   _ -->
<!--  / __(_) |_ __ _| |_(_)___ _ _ -->
<!-- | (__| |  _/ _` |  _| / _ \ ' \ -->
<!--  \___|_|\__\__,_|\__|_\___/_||_| -->

<h3>Citation</h3>
<p style="text-align:justify">
  For more details and additional results, <a href="https://arxiv.org/abs/2302.13153" style="text-decoration:none">read the full paper</a>.
</p>
<code>
  @misc{ma2023directed,<br/>
  <div style="padding-left: 5%; margin: 0">
      title={Directed Diffusion: Direct Control of Object Placement through Attention Guidance},<br/>
      author={Wan-Duo Kurt Ma and J. P. Lewis and Avisek Lahiri and W. Bastiaan Kleijn and Thomas Leung},<br/>
      year={2023},<br/>
      eprint={2302.13153},<br/>
      archivePrefix={arXiv},<br/>
      primaryClass={cs.CV}<br/>
  </div>
  }<br/>
</code>

<span style="margin-bottom: 10%"></span>
</d-article>


</body>
